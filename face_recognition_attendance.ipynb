{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup (RUN THIS CELL FIRST)\n",
        "# - Creates folder structure\n",
        "# - Checks and prints library versions\n",
        "# - Downloads Haar Cascade for face detection if missing\n",
        "# - Initializes attendance.csv with headers if missing\n",
        "# - Defines global constants used later\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import urllib.request\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import sklearn\n",
        "\n",
        "# Print environment info\n",
        "print(\"Environment info:\")\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"OpenCV: {cv2.__version__}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"Pandas: {pd.__version__}\")\n",
        "print(f\"Matplotlib: {matplotlib.__version__}\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "\n",
        "# Define base paths\n",
        "BASE_DIR = Path.cwd()\n",
        "PROJECT_DIR = BASE_DIR  # notebook stored under attendance_system/\n",
        "DATASET_DIR = PROJECT_DIR / \"dataset\"\n",
        "MODELS_DIR = PROJECT_DIR / \"models\"\n",
        "ATTENDANCE_CSV = PROJECT_DIR / \"attendance.csv\"\n",
        "HAAR_PATH = MODELS_DIR / \"haarcascade_frontalface_default.xml\"\n",
        "\n",
        "# Constants\n",
        "IMG_SIZE = 64  # resize to 64x64\n",
        "PCA_COMPONENTS = 100\n",
        "MAX_PEOPLE = 10\n",
        "\n",
        "print(\"\\nSetting up folders...\")\n",
        "for p in [PROJECT_DIR, DATASET_DIR, MODELS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Project dir: {PROJECT_DIR}\")\n",
        "print(f\"Dataset dir: {DATASET_DIR}\")\n",
        "print(f\"Models dir: {MODELS_DIR}\")\n",
        "\n",
        "# Optionally pre-create person subfolders (person1..person10)\n",
        "for i in range(1, MAX_PEOPLE + 1):\n",
        "    person_dir = DATASET_DIR / f\"person{i}\"\n",
        "    person_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Download Haar Cascade if not present\n",
        "if not HAAR_PATH.exists():\n",
        "    print(\"Haar cascade not found. Downloading...\")\n",
        "    try:\n",
        "        url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "        urllib.request.urlretrieve(url, str(HAAR_PATH))\n",
        "        print(f\"Downloaded Haar cascade to: {HAAR_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(\"ERROR: Failed to download Haar cascade. Please ensure internet access or add the file manually.\")\n",
        "        print(f\"Details: {e}\")\n",
        "else:\n",
        "    print(f\"Haar cascade present: {HAAR_PATH}\")\n",
        "\n",
        "# Initialize attendance.csv if missing\n",
        "if not ATTENDANCE_CSV.exists():\n",
        "    print(\"Creating attendance.csv with headers [name,date,time]...\")\n",
        "    try:\n",
        "        df = pd.DataFrame(columns=[\"name\", \"date\", \"time\"])  # empty with headers\n",
        "        df.to_csv(ATTENDANCE_CSV, index=False)\n",
        "        print(f\"Created: {ATTENDANCE_CSV}\")\n",
        "    except Exception as e:\n",
        "        print(\"ERROR: Could not create attendance.csv\")\n",
        "        print(f\"Details: {e}\")\n",
        "else:\n",
        "    print(f\"Found existing attendance.csv: {ATTENDANCE_CSV}\")\n",
        "\n",
        "print(\"\\nSetup complete. Proceed to the next cell after confirming this ran without errors.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Utilities - Detection, Preprocessing, Dataset Helpers (RUN THIS CELL FIRST IN NEW SESSIONS)\n",
        "# - Face detection using Haar Cascade\n",
        "# - Preprocessing: grayscale, histogram equalization, resize to 64x64\n",
        "# - Dataset scanning and loading with labels\n",
        "# - Safe image reading and error handling\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Reuse globals from Cell 1: HAAR_PATH, DATASET_DIR, IMG_SIZE\n",
        "\n",
        "# Load Haar Cascade\n",
        "_face_cascade = None\n",
        "\n",
        "def get_face_cascade():\n",
        "    global _face_cascade\n",
        "    if _face_cascade is None:\n",
        "        if not Path(HAAR_PATH).exists():\n",
        "            raise FileNotFoundError(f\"Haar cascade not found at {HAAR_PATH}. Run Cell 1 or place the file.\")\n",
        "        _face_cascade = cv2.CascadeClassifier(str(HAAR_PATH))\n",
        "        if _face_cascade.empty():\n",
        "            raise RuntimeError(\"Failed to load Haar cascade. The file may be corrupt.\")\n",
        "    return _face_cascade\n",
        "\n",
        "\n",
        "def detect_faces_gray(image_bgr: np.ndarray, scaleFactor: float = 1.3, minNeighbors: int = 5) -> List[Tuple[int, int, int, int]]:\n",
        "    \"\"\"Detect faces on a BGR image and return list of (x, y, w, h).\"\"\"\n",
        "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    faces = get_face_cascade().detectMultiScale(gray, scaleFactor=scaleFactor, minNeighbors=minNeighbors)\n",
        "    return [(int(x), int(y), int(w), int(h)) for (x, y, w, h) in faces]\n",
        "\n",
        "\n",
        "def preprocess_face_crop(image_bgr: np.ndarray, bbox: Tuple[int, int, int, int]) -> np.ndarray:\n",
        "    \"\"\"Given a BGR image and a face bbox (x,y,w,h), return processed 64x64 grayscale face.\"\"\"\n",
        "    x, y, w, h = bbox\n",
        "    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    face = gray[y:y+h, x:x+w]\n",
        "    if face.size == 0:\n",
        "        raise ValueError(\"Empty face crop. Check bbox.\")\n",
        "    # Histogram equalization and resize\n",
        "    face = cv2.equalizeHist(face)\n",
        "    face = cv2.resize(face, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "    return face\n",
        "\n",
        "\n",
        "def preprocess_image_path(image_path: Path) -> List[np.ndarray]:\n",
        "    \"\"\"Read an image path safely, detect faces, and return list of processed faces (64x64).\"\"\"\n",
        "    img = cv2.imread(str(image_path))\n",
        "    if img is None:\n",
        "        print(f\"WARN: Could not read image: {image_path}\")\n",
        "        return []\n",
        "    bboxes = detect_faces_gray(img)\n",
        "    if len(bboxes) == 0:\n",
        "        # As a fallback, try entire image if no faces detected (for synthetic samples)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        gray = cv2.equalizeHist(gray)\n",
        "        resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_AREA)\n",
        "        return [resized]\n",
        "    faces = []\n",
        "    for bbox in bboxes:\n",
        "        try:\n",
        "            faces.append(preprocess_face_crop(img, bbox))\n",
        "        except Exception as e:\n",
        "            print(f\"WARN: Failed to preprocess face in {image_path}: {e}\")\n",
        "    return faces\n",
        "\n",
        "\n",
        "def list_people(dataset_dir: Path) -> List[str]:\n",
        "    people = []\n",
        "    for entry in sorted(dataset_dir.iterdir()):\n",
        "        if entry.is_dir():\n",
        "            people.append(entry.name)\n",
        "    return people\n",
        "\n",
        "\n",
        "def load_dataset(dataset_dir: Path) -> Tuple[np.ndarray, np.ndarray, Dict[int, str]]:\n",
        "    \"\"\"Scan dataset folders, preprocess faces, and build X, y and id->name mapping.\n",
        "    - X: (N, IMG_SIZE*IMG_SIZE) grayscale flattened\n",
        "    - y: (N,) integer labels\n",
        "    - id_to_name: dict mapping label id to folder name\n",
        "    \"\"\"\n",
        "    people = list_people(dataset_dir)\n",
        "    if len(people) == 0:\n",
        "        print(f\"NOTE: No person folders found in {dataset_dir}. You can run the sample data cell next.\")\n",
        "    id_to_name = {i: name for i, name in enumerate(people)}\n",
        "    name_to_id = {name: i for i, name in id_to_name.items()}\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    for name in people:\n",
        "        person_dir = dataset_dir / name\n",
        "        image_files = [p for p in person_dir.iterdir() if p.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp'}]\n",
        "        if len(image_files) == 0:\n",
        "            print(f\"WARN: No images in {person_dir}\")\n",
        "        for img_path in image_files:\n",
        "            faces = preprocess_image_path(img_path)\n",
        "            for face in faces:\n",
        "                X_list.append(face.flatten().astype(np.float32) / 255.0)\n",
        "                y_list.append(name_to_id[name])\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        print(\"WARN: Dataset appears empty after preprocessing.\")\n",
        "        return np.empty((0, IMG_SIZE * IMG_SIZE), dtype=np.float32), np.empty((0,), dtype=np.int64), id_to_name\n",
        "\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    y = np.array(y_list, dtype=np.int64)\n",
        "    print(f\"Loaded dataset: X={X.shape}, y={y.shape}, classes={len(id_to_name)}\")\n",
        "    return X, y, id_to_name\n",
        "\n",
        "print(\"Utilities loaded. You can proceed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Sample Data Creation (optional, for testing) - Creates simple synthetic faces\n",
        "# - Generates 10 folders person1..person10 if not present\n",
        "# - Creates 5 synthetic 64x64 grayscale images per person\n",
        "# - These are simplistic and only for pipeline testing; replace with real images later\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "NUM_PEOPLE = 10\n",
        "IMAGES_PER_PERSON = 5\n",
        "\n",
        "created = 0\n",
        "for i in range(1, NUM_PEOPLE + 1):\n",
        "    person_dir = DATASET_DIR / f\"person{i}\"\n",
        "    person_dir.mkdir(exist_ok=True)\n",
        "    existing = list(person_dir.glob(\"*.png\")) + list(person_dir.glob(\"*.jpg\"))\n",
        "    to_create = max(0, IMAGES_PER_PERSON - len(existing))\n",
        "    for j in range(to_create):\n",
        "        # Simple synthetic pattern with noise, circle/ellipse variations\n",
        "        img = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
        "        cv2.circle(img, (IMG_SIZE//2, IMG_SIZE//2), 20 + (i % 5), 150 + (i*8) % 100, -1)\n",
        "        cv2.ellipse(img, (IMG_SIZE//2, IMG_SIZE//2 + 10), (10 + (j%3)*3, 5 + (j%3)*2), 0, 0, 360, 80 + (j*20), -1)\n",
        "        noise = (np.random.randn(IMG_SIZE, IMG_SIZE) * 8).astype(np.int16)\n",
        "        noisy = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
        "        out = cv2.equalizeHist(noisy)\n",
        "        out_path = person_dir / f\"synthetic_{j+1}.png\"\n",
        "        cv2.imwrite(str(out_path), out)\n",
        "        created += 1\n",
        "\n",
        "print(f\"Synthetic images created: {created}\")\n",
        "print(\"If you have real face images, place 5-10 images per person folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load dataset and train/test split\n",
        "# - Loads images from dataset/, preprocesses faces\n",
        "# - Builds X (flattened 64x64) and y (labels)\n",
        "# - Train/test split with stratification\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load\n",
        "X, y, id_to_name = load_dataset(DATASET_DIR)\n",
        "name_to_id = {v: k for k, v in id_to_name.items()}\n",
        "print(\"Label map:\", id_to_name)\n",
        "\n",
        "if X.shape[0] == 0 or len(id_to_name) == 0:\n",
        "    raise RuntimeError(\"Dataset is empty. Add images or run Cell 3 to generate samples.\")\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y if len(np.unique(y)) > 1 else None\n",
        ")\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: PCA (100 components) fit on training set and transform train/test\n",
        "# - Caps components by min(n_samples, n_features)\n",
        "# - Uses efficient solver selection\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import joblib\n",
        "\n",
        "max_components = min(PCA_COMPONENTS, X_train.shape[1], X_train.shape[0])\n",
        "if max_components <= 0:\n",
        "    raise RuntimeError(\"PCA cannot run: not enough training samples or features.\")\n",
        "\n",
        "# Use randomized when components are less than min(n_samples, n_features) for speed\n",
        "svd_solver = 'randomized' if max_components < min(X_train.shape[0], X_train.shape[1]) else 'full'\n",
        "\n",
        "pca = PCA(n_components=max_components, svd_solver=svd_solver, whiten=False, random_state=42)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "print(f\"PCA fitted. Components: {pca.n_components_}, solver: {svd_solver}\")\n",
        "print(f\"Shapes -> train: {X_train_pca.shape}, test: {X_test_pca.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Train and compare classifiers (SVM, Logistic Regression, Decision Tree, Random Forest)\n",
        "# - Trains each on PCA features\n",
        "# - Reports accuracy and confidence proxy (decision_function/proba max)\n",
        "# - Selects best model\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "models = {\n",
        "    \"SVM_RBF\": SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42),\n",
        "    \"LogReg\": LogisticRegression(max_iter=2000, multi_class='auto', solver='lbfgs'),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=None, random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, clf in models.items():\n",
        "    clf.fit(X_train_pca, y_train)\n",
        "    y_pred = clf.predict(X_test_pca)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[name] = {\n",
        "        \"model\": clf,\n",
        "        \"accuracy\": acc\n",
        "    }\n",
        "    print(f\"{name} accuracy: {acc:.3f}\")\n",
        "\n",
        "# Pick best by accuracy\n",
        "best_name = max(results, key=lambda k: results[k][\"accuracy\"])\n",
        "best_model = results[best_name][\"model\"]\n",
        "print(f\"\\nBest model: {best_name} (accuracy={results[best_name]['accuracy']:.3f})\")\n",
        "\n",
        "# Example: show per-class report for the best model\n",
        "print(\"\\nClassification report for best model:\")\n",
        "best_pred = best_model.predict(X_test_pca)\n",
        "print(classification_report(y_test, best_pred, target_names=[id_to_name[i] for i in sorted(id_to_name.keys())]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Save best model, PCA and label mapping\n",
        "# - Saves to models/ directory\n",
        "\n",
        "import json\n",
        "import joblib\n",
        "\n",
        "BEST_MODEL_PATH = MODELS_DIR / \"best_model.joblib\"\n",
        "PCA_PATH = MODELS_DIR / \"pca.joblib\"\n",
        "LABELS_PATH = MODELS_DIR / \"labels.json\"\n",
        "\n",
        "joblib.dump(best_model, BEST_MODEL_PATH)\n",
        "joblib.dump(pca, PCA_PATH)\n",
        "with open(LABELS_PATH, 'w') as f:\n",
        "    json.dump(id_to_name, f)\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(BEST_MODEL_PATH)\n",
        "print(PCA_PATH)\n",
        "print(LABELS_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Load saved model/PCA and verify predictions with confidence scores\n",
        "# - Demonstrates loading artifacts and computing prediction + confidence\n",
        "\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "with open(LABELS_PATH, 'r') as f:\n",
        "    id_to_name_loaded = json.load(f)\n",
        "name_list = [id_to_name_loaded[str(i)] if isinstance(i, int) else id_to_name_loaded[i] for i in sorted(map(int, id_to_name_loaded.keys()))]\n",
        "\n",
        "model_loaded = joblib.load(BEST_MODEL_PATH)\n",
        "pca_loaded = joblib.load(PCA_PATH)\n",
        "\n",
        "# Use a few test samples\n",
        "if X_test.shape[0] > 0:\n",
        "    Z = pca_loaded.transform(X_test[:5])\n",
        "    probs = None\n",
        "    if hasattr(model_loaded, 'predict_proba'):\n",
        "        probs = model_loaded.predict_proba(Z)\n",
        "        conf = probs.max(axis=1)\n",
        "    else:\n",
        "        # decision_function fallback -> convert to pseudo-confidence via softmax\n",
        "        dec = model_loaded.decision_function(Z)\n",
        "        # handle binary vs multiclass\n",
        "        if dec.ndim == 1:\n",
        "            dec = np.vstack([-dec, dec]).T\n",
        "        e = np.exp(dec - dec.max(axis=1, keepdims=True))\n",
        "        probs = e / e.sum(axis=1, keepdims=True)\n",
        "        conf = probs.max(axis=1)\n",
        "    preds = model_loaded.predict(Z)\n",
        "    print(\"Predictions (first 5 test samples):\")\n",
        "    for i, (yp, c) in enumerate(zip(preds, conf)):\n",
        "        print(f\"idx {i}: pred={id_to_name[int(yp)]} confidence={float(c):.3f}\")\n",
        "else:\n",
        "    print(\"No test samples available for verification.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Real-time webcam recognition with attendance logging\n",
        "# - Press 'q' to quit\n",
        "# - Draws bounding boxes and predicted names with confidence\n",
        "# - Logs attendance to attendance.csv once per person per session\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# Load artifacts\n",
        "model = joblib.load(BEST_MODEL_PATH)\n",
        "pca_rt = joblib.load(PCA_PATH)\n",
        "with open(LABELS_PATH, 'r') as f:\n",
        "    id_to_name_rt = json.load(f)\n",
        "\n",
        "# Reverse map\n",
        "id_to_name_rt = {int(k): v for k, v in id_to_name_rt.items()}\n",
        "\n",
        "# Session attendance memory to avoid duplicates\n",
        "marked = set()\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError(\"ERROR: Cannot access webcam. Ensure a camera is connected and not in use.\")\n",
        "\n",
        "print(\"Webcam open. Press 'q' to quit.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            print(\"WARN: Failed to read frame from webcam.\")\n",
        "            break\n",
        "\n",
        "        bboxes = detect_faces_gray(frame)\n",
        "        for (x, y, w, h) in bboxes:\n",
        "            try:\n",
        "                face_img = preprocess_face_crop(frame, (x, y, w, h))\n",
        "                x_input = (face_img.flatten().astype(np.float32) / 255.0)[None, :]\n",
        "                z = pca_rt.transform(x_input)\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    prob = model.predict_proba(z)[0]\n",
        "                    conf = float(prob.max())\n",
        "                    pred_id = int(prob.argmax())\n",
        "                else:\n",
        "                    dec = model.decision_function(z)\n",
        "                    if dec.ndim == 1:\n",
        "                        dec = np.vstack([-dec, dec]).T\n",
        "                    e = np.exp(dec - dec.max(axis=1, keepdims=True))\n",
        "                    prob = (e / e.sum(axis=1, keepdims=True))[0]\n",
        "                    conf = float(prob.max())\n",
        "                    pred_id = int(prob.argmax())\n",
        "                name = id_to_name_rt.get(pred_id, \"unknown\")\n",
        "\n",
        "                # Draw\n",
        "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "                label = f\"{name} ({conf:.2f})\"\n",
        "                cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
        "\n",
        "                # Attendance logging (confidence threshold to reduce false positives)\n",
        "                if conf >= 0.5 and name != \"unknown\" and name not in marked:\n",
        "                    now = datetime.now()\n",
        "                    row = {\"name\": name, \"date\": now.strftime(\"%Y-%m-%d\"), \"time\": now.strftime(\"%H:%M:%S\")}\n",
        "                    try:\n",
        "                        df = pd.read_csv(ATTENDANCE_CSV)\n",
        "                        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
        "                        df.to_csv(ATTENDANCE_CSV, index=False)\n",
        "                        print(f\"Attendance marked: {row}\")\n",
        "                        marked.add(name)\n",
        "                    except Exception as e:\n",
        "                        print(f\"ERROR writing attendance: {e}\")\n",
        "            except Exception as e:\n",
        "                # ignore per-face errors, continue\n",
        "                pass\n",
        "\n",
        "        cv2.imshow('Face Recognition Attendance', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "finally:\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Webcam closed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Attendance viewer and helpers\n",
        "# - Load and display last N rows\n",
        "# - Plot simple counts per person (matplotlib)\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    df_att = pd.read_csv(ATTENDANCE_CSV)\n",
        "except Exception as e:\n",
        "    print(f\"ERROR reading attendance.csv: {e}\")\n",
        "    df_att = pd.DataFrame(columns=[\"name\",\"date\",\"time\"])\n",
        "\n",
        "print(\"Last 10 attendance records:\")\n",
        "print(df_att.tail(10))\n",
        "\n",
        "if len(df_att) > 0:\n",
        "    counts = df_att.groupby('name').size().sort_values(ascending=False)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    counts.plot(kind='bar')\n",
        "    plt.title('Attendance Counts by Person')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('Person')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No attendance records to plot yet.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Create or reset attendance.csv (RUN ONLY IF NEEDED)\n",
        "# - Safely creates attendance.csv with headers [name,date,time]\n",
        "# - Use this if the file is missing or you want to clear it\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    df = pd.DataFrame(columns=[\"name\", \"date\", \"time\"])  # empty with headers\n",
        "    df.to_csv(ATTENDANCE_CSV, index=False)\n",
        "    print(f\"attendance.csv created/reset at: {ATTENDANCE_CSV}\")\n",
        "except Exception as e:\n",
        "    print(\"ERROR: Could not create/reset attendance.csv\")\n",
        "    print(f\"Details: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Create dataset_index.csv from dataset/ (optional)\n",
        "# - Scans dataset/<person>/*.{jpg,png,jpeg,bmp}\n",
        "# - Writes CSV with columns: path, person\n",
        "# - Helpful for auditing or editing the dataset via spreadsheet\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "rows = []\n",
        "for person_dir in sorted(DATASET_DIR.iterdir()):\n",
        "    if not person_dir.is_dir():\n",
        "        continue\n",
        "    person = person_dir.name\n",
        "    for p in person_dir.iterdir():\n",
        "        if p.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp'}:\n",
        "            rows.append({\"path\": str(p.resolve()), \"person\": person})\n",
        "\n",
        "df_index = pd.DataFrame(rows, columns=[\"path\", \"person\"])\n",
        "index_path = (PROJECT_DIR / \"dataset_index.csv\")\n",
        "df_index.to_csv(index_path, index=False)\n",
        "print(f\"dataset_index.csv created with {len(df_index)} rows at: {index_path}\")\n",
        "print(df_index.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Hyperparameter Tuning with GridSearchCV (SVM and RandomForest)\n",
        "# - Runs CV-based grid search on PCA features\n",
        "# - Compares tuned models to current best; updates best if improved\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Grids (kept small for quick runs; expand if needed)\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 0.01, 0.1, 1.0],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "print(\"Tuning SVM...\")\n",
        "svm_gs = GridSearchCV(\n",
        "    SVC(probability=True, random_state=42),\n",
        "    svm_param_grid,\n",
        "    cv=min(5, len(np.unique(y_train))),\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "svm_gs.fit(X_train_pca, y_train)\n",
        "print(\"SVM best params:\", svm_gs.best_params_)\n",
        "svm_pred = svm_gs.best_estimator_.predict(X_test_pca)\n",
        "svm_acc = accuracy_score(y_test, svm_pred)\n",
        "print(f\"SVM tuned test accuracy: {svm_acc:.3f}\")\n",
        "\n",
        "print(\"\\nTuning RandomForest...\")\n",
        "rf_gs = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    rf_param_grid,\n",
        "    cv=min(5, len(np.unique(y_train))),\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "rf_gs.fit(X_train_pca, y_train)\n",
        "print(\"RF best params:\", rf_gs.best_params_)\n",
        "rf_pred = rf_gs.best_estimator_.predict(X_test_pca)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "print(f\"RF tuned test accuracy: {rf_acc:.3f}\")\n",
        "\n",
        "# Compare vs previous best\n",
        "current_best_name = best_name\n",
        "current_best_acc = results[best_name]['accuracy'] if 'results' in globals() and best_name in results else -1\n",
        "\n",
        "candidates = [\n",
        "    (\"SVM_TUNED\", svm_gs.best_estimator_, svm_acc),\n",
        "    (\"RF_TUNED\", rf_gs.best_estimator_, rf_acc)\n",
        "]\n",
        "\n",
        "for cand_name, cand_model, cand_acc in candidates:\n",
        "    print(f\"Candidate {cand_name} accuracy: {cand_acc:.3f}\")\n",
        "\n",
        "# Select improved best\n",
        "all_options = candidates + [(current_best_name, best_model, current_best_acc)]\n",
        "new_best_name, new_best_model, new_best_acc = max(all_options, key=lambda t: t[2])\n",
        "\n",
        "if new_best_acc > current_best_acc:\n",
        "    print(f\"\\nNew best model: {new_best_name} with accuracy {new_best_acc:.3f} (prev {current_best_name}: {current_best_acc:.3f})\")\n",
        "    best_name = new_best_name\n",
        "    best_model = new_best_model\n",
        "    # Save improved model\n",
        "    joblib.dump(best_model, BEST_MODEL_PATH)\n",
        "    print(f\"Updated saved best model at: {BEST_MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"\\nBest model unchanged: {current_best_name} with accuracy {current_best_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Robust Hyperparameter Tuning (StratifiedKFold with safe n_splits)\n",
        "# - Avoids CV errors when some classes have few samples\n",
        "# - Uses the same grids but caps folds by smallest class count\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Determine safe number of folds\n",
        "class_counts = Counter(y_train.tolist())\n",
        "min_class_count = min(class_counts.values())\n",
        "safe_folds = max(2, min(5, min_class_count))  # at least 2 folds, at most 5, not exceeding smallest class\n",
        "print(f\"Class counts: {dict(class_counts)} | Using {safe_folds}-fold StratifiedKFold\")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=safe_folds, shuffle=True, random_state=42)\n",
        "\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 0.01, 0.1, 1.0],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "print(\"Tuning SVM (robust)...\")\n",
        "svm_gs = GridSearchCV(\n",
        "    SVC(probability=True, random_state=42),\n",
        "    svm_param_grid,\n",
        "    cv=skf,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy',\n",
        "    error_score='raise'\n",
        ")\n",
        "svm_gs.fit(X_train_pca, y_train)\n",
        "print(\"SVM best params:\", svm_gs.best_params_)\n",
        "svm_pred = svm_gs.best_estimator_.predict(X_test_pca)\n",
        "svm_acc = accuracy_score(y_test, svm_pred)\n",
        "print(f\"SVM tuned test accuracy: {svm_acc:.3f}\")\n",
        "\n",
        "print(\"\\nTuning RandomForest (robust)...\")\n",
        "rf_gs = GridSearchCV(\n",
        "    RandomForestClassifier(random_state=42),\n",
        "    rf_param_grid,\n",
        "    cv=skf,\n",
        "    n_jobs=-1,\n",
        "    scoring='accuracy',\n",
        "    error_score='raise'\n",
        ")\n",
        "rf_gs.fit(X_train_pca, y_train)\n",
        "print(\"RF best params:\", rf_gs.best_params_)\n",
        "rf_pred = rf_gs.best_estimator_.predict(X_test_pca)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "print(f\"RF tuned test accuracy: {rf_acc:.3f}\")\n",
        "\n",
        "# Compare vs previous best\n",
        "current_best_name = best_name\n",
        "current_best_acc = results[best_name]['accuracy'] if 'results' in globals() and best_name in results else -1\n",
        "\n",
        "candidates = [\n",
        "    (\"SVM_TUNED_SAFE\", svm_gs.best_estimator_, svm_acc),\n",
        "    (\"RF_TUNED_SAFE\", rf_gs.best_estimator_, rf_acc)\n",
        "]\n",
        "\n",
        "for cand_name, cand_model, cand_acc in candidates:\n",
        "    print(f\"Candidate {cand_name} accuracy: {cand_acc:.3f}\")\n",
        "\n",
        "# Select improved best\n",
        "all_options = candidates + [(current_best_name, best_model, current_best_acc)]\n",
        "new_best_name, new_best_model, new_best_acc = max(all_options, key=lambda t: t[2])\n",
        "\n",
        "if new_best_acc > current_best_acc:\n",
        "    print(f\"\\nNew best model: {new_best_name} with accuracy {new_best_acc:.3f} (prev {current_best_name}: {current_best_acc:.3f})\")\n",
        "    best_name = new_best_name\n",
        "    best_model = new_best_model\n",
        "    joblib.dump(best_model, BEST_MODEL_PATH)\n",
        "    print(f\"Updated saved best model at: {BEST_MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"\\nBest model unchanged: {current_best_name} with accuracy {current_best_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: Capture your own images to dataset/<your_name>\n",
        "# - Enter a folder/person name (e.g., your real name)\n",
        "# - Captures face crops using Haar cascade and saves 64x64 grayscale images\n",
        "# - Press 'c' to capture, 'q' to quit\n",
        "\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "person_name = input(\"Enter person/folder name (e.g., 'alice'): \").strip()\n",
        "if not person_name:\n",
        "    raise ValueError(\"Name cannot be empty.\")\n",
        "\n",
        "save_dir = DATASET_DIR / person_name\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Saving images to: {save_dir}\")\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    raise RuntimeError(\"ERROR: Cannot access webcam.\")\n",
        "\n",
        "count = 0\n",
        "print(\"Press 'c' to capture face, 'q' to quit.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        ok, frame = cap.read()\n",
        "        if not ok:\n",
        "            print(\"WARN: Could not read frame.\")\n",
        "            break\n",
        "        bboxes = detect_faces_gray(frame, scaleFactor=1.2, minNeighbors=5)\n",
        "        for (x, y, w, h) in bboxes:\n",
        "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        cv2.imshow('Capture', frame)\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "        if key == ord('c'):\n",
        "            if len(bboxes) == 0:\n",
        "                print(\"No face detected. Try again.\")\n",
        "                continue\n",
        "            # take the first detected face\n",
        "            (x, y, w, h) = bboxes[0]\n",
        "            face_img = preprocess_face_crop(frame, (x, y, w, h))\n",
        "            out_path = save_dir / f\"img_{count+1:03d}.png\"\n",
        "            cv2.imwrite(str(out_path), face_img)\n",
        "            count += 1\n",
        "            print(f\"Saved: {out_path}\")\n",
        "finally:\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(f\"Capture finished. Total saved: {count}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
